#!/usr/bin/env groovy

@Library('apm@current') _

pipeline {
  agent { label 'ubuntu-20.04 && immutable && docker' }
  environment {
    REPO = 'e2e-testing'
    BASE_DIR = "src/github.com/elastic/${env.REPO}"
    ELASTIC_APM_ACTIVE="true"
    ELASTIC_APM_ENVIRONMENT="ci"
    ELASTIC_APM_LOG_FILE="stderr"
    ELASTIC_APM_LOG_LEVEL="debug"
    NIGHTLY_TAG="@nightly"
    NOTIFY_TO = credentials('notify-to')
    JOB_GCS_BUCKET = credentials('gcs-bucket')
    JOB_GIT_CREDENTIALS = "2a9602aa-ab9f-4e52-baf3-b71ca88469c7-UserAndToken"
    DOCKER_ELASTIC_SECRET = 'secret/observability-team/ci/docker-registry/prod'
    DOCKER_REGISTRY = 'docker.elastic.co'
    ELASTIC_CLOUD_SECRET = 'secret/observability-team/ci/elastic-cloud/observability-team-user'
    GCP_PROVISIONER_SECRET = 'secret/observability-team/ci/service-account/jenkins-gce-provisioner'
    AWS_PROVISIONER_SECRET = 'secret/observability-team/ci/elastic-observability-aws-account-auth'
    TEST_MATRIX_FILE = "${params.testMatrixFile}"
  }
  options {
    timeout(time: 1, unit: 'HOURS')
    buildDiscarder(logRotator(numToKeepStr: '20', artifactNumToKeepStr: '20', daysToKeepStr: '30'))
    timestamps()
    ansiColor('xterm')
    disableResume()
    durabilityHint('PERFORMANCE_OPTIMIZED')
    rateLimitBuilds(throttle: [count: 60, durationName: 'hour', userBoost: true])
    quietPeriod(10)
  }
  triggers {
    issueCommentTrigger("${obltGitHubComments()}")
  }
  parameters {
    booleanParam(name: 'Run_As_Master_Branch', defaultValue: false, description: 'Allow to run any steps on a PR, some steps normally only run on master branch.')
    booleanParam(name: "SKIP_SCENARIOS", defaultValue: true, description: "If it's needed to skip those scenarios marked as @skip. Default true")
    booleanParam(name: "NIGHTLY_SCENARIOS", defaultValue: false, description: "If it's needed to include the scenarios marked as @nightly in the test execution. Default false")
    string(name: 'runTestsSuites', defaultValue: '', description: 'A comma-separated list of test suites to run (default: empty to run all test suites)')
    string(name: 'testMatrixFile', defaultValue: '.ci/.e2e-tests.yaml', description: 'The file with the test suite and scenarios to be tested.')
    booleanParam(name: "forceSkipGitChecks", defaultValue: false, description: "If it's needed to check for Git changes to filter by modified sources")
    booleanParam(name: "forceSkipPresubmit", defaultValue: false, description: "If it's needed to execute the pre-submit tests: unit and precommit.")
    booleanParam(name: "notifyOnGreenBuilds", defaultValue: false, description: "If it's needed to notify to Slack with green builds.")
    string(name: 'SLACK_CHANNEL', defaultValue: 'observablt-bots', description: 'The Slack channel(s) where errors will be posted. For multiple channels, use a comma-separated list of channels')
    string(name: 'ELASTIC_AGENT_DOWNLOAD_URL', defaultValue: '', description: 'If present, it will override the download URL for the Elastic agent artifact. (I.e. https://snapshots.elastic.co/8.0.0-59098054/downloads/beats/elastic-agent/elastic-agent-8.0.0-SNAPSHOT-linux-x86_64.tar.gz')
    string(name: 'BEAT_VERSION', defaultValue: '8.0.0-0da7fd6d-SNAPSHOT', description: 'SemVer version of the Beat to be used for the tests. You can use here the tag of your PR to test your changes')
    string(name: 'ELASTIC_AGENT_STALE_VERSION', defaultValue: '7.15-SNAPSHOT', description: 'SemVer version of the stale stand-alone elastic-agent to be used for Fleet upgrade tests.')
    choice(name: 'LOG_LEVEL', choices: ['DEBUG', 'TRACE', 'INFO'], description: 'Log level to be used')
    choice(name: 'TIMEOUT_FACTOR', choices: ['5', '3', '7', '11'], description: 'Max number of minutes for timeout backoff strategies')
    string(name: 'KIBANA_VERSION', defaultValue: '', description: 'Docker tag of the kibana to be used for the tests. It will refer to an image related to a Kibana PR, under the Observability-CI namespace')
    string(name: 'STACK_VERSION', defaultValue: '8.0.0-0da7fd6d-SNAPSHOT', description: 'SemVer version of the stack to be used for the tests.')
    string(name: 'HELM_CHART_VERSION', defaultValue: '7.11.2', description: 'SemVer version of Helm chart to be used.')
    string(name: 'HELM_VERSION', defaultValue: '3.5.2', description: 'SemVer version of Helm to be used.')
    string(name: 'KIND_VERSION', defaultValue: '0.10.0', description: 'SemVer version of Kind to be used.')
    string(name: 'KUBERNETES_VERSION', defaultValue: '1.18.2', description: 'SemVer version of Kubernetes to be used.')
    string(name: 'GITHUB_CHECK_NAME', defaultValue: '', description: 'Name of the GitHub check to be updated. Only if this build is triggered from another parent stream.')
    string(name: 'GITHUB_CHECK_REPO', defaultValue: '', description: 'Name of the GitHub repo to be updated. Only if this build is triggered from another parent stream.')
    string(name: 'GITHUB_CHECK_SHA1', defaultValue: '', description: 'Git SHA for the Beats upstream project (branch or PR)')
  }
  stages {
    stage('Initializing'){
      options { skipDefaultCheckout() }
      environment {
        HOME = "${env.WORKSPACE}"
        PATH = "${env.PATH}:${env.WORKSPACE}/bin:${env.WORKSPACE}/${env.BASE_DIR}/.ci/scripts"
        GO111MODULE = 'on'
        SKIP_SCENARIOS = "${params.SKIP_SCENARIOS}"
        NIGHTLY_SCENARIOS = "${params.NIGHTLY_SCENARIOS}"
        SLACK_CHANNEL = "${params.SLACK_CHANNEL.trim()}"
        ELASTIC_AGENT_DOWNLOAD_URL = "${params.ELASTIC_AGENT_DOWNLOAD_URL.trim()}"
        BEAT_VERSION = "${params.BEAT_VERSION.trim()}"
        KIBANA_VERSION = "${params.KIBANA_VERSION.trim()}"
        STACK_VERSION = "${params.STACK_VERSION.trim()}"
        FORCE_SKIP_GIT_CHECKS = "${params.forceSkipGitChecks}"
        FORCE_SKIP_PRESUBMIT = "${params.forceSkipPresubmit}"
        HELM_CHART_VERSION = "${params.HELM_CHART_VERSION.trim()}"
        HELM_VERSION = "${params.HELM_VERSION.trim()}"
        KIND_VERSION = "${params.KIND_VERSION.trim()}"
        KUBERNETES_VERSION = "${params.KUBERNETES_VERSION.trim()}"
        LOG_LEVEL = "${params.LOG_LEVEL.trim()}"
        TIMEOUT_FACTOR = "${params.TIMEOUT_FACTOR.trim()}"
      }
      stages {
        stage('Checkout') {
          steps {
            pipelineManager([ cancelPreviousRunningBuilds: [ when: 'PR' ] ])
            deleteDir()
            gitCheckout(basedir: BASE_DIR, githubNotifyFirstTimeContributor: true)
            githubCheckNotify('PENDING')  // we want to notify the upstream about the e2e the soonest
            stash allowEmpty: true, name: 'source', useDefaultExcludes: false
            setEnvVar("GO_VERSION", readFile("${env.WORKSPACE}/${env.BASE_DIR}/.go-version").trim())
            dir("${BASE_DIR}"){
              // Skip all the test stages for PR's with markdown changes only
              setEnvVar("SKIP_TESTS", isGitRegionMatch(patterns: [ '.*\\.md' ], shouldMatchAll: true))
            }
          }
        }
        // stage('Pre-Submit') {
        //   when {
        //     beforeAgent true
        //     expression { return env.FORCE_SKIP_PRESUBMIT == "false" }
        //   }
        //   parallel {
        //     stage('Sanity checks') {
        //       agent { label 'ubuntu-18.04 && immutable && docker' }
        //       environment {
        //         PATH = "${env.WORKSPACE}/${env.BASE_DIR}/bin:${env.PATH}"
        //         GO111MODULE = 'auto'
        //       }
        //       options { skipDefaultCheckout() }
        //       steps {
        //         withGithubNotify(context: 'Sanity checks', tab: 'tests') {
        //           deleteDir()
        //           unstash 'source'
        //           withGoEnv(version: "${GO_VERSION}"){
        //             dir(BASE_DIR){
        //               retryWithSleep(retries: 2, seconds: 5, backoff: true){ sh script: '.ci/scripts/install-dependencies.sh', label: 'Install dependencies' }
        //               preCommit(commit: "${GIT_BASE_COMMIT}", junit: true)
        //             }
        //           }
        //         }
        //       }
        //     }
        //     stage('Unit Tests') {
        //       options { skipDefaultCheckout() }
        //       when {
        //         beforeAgent true
        //         expression { return env.SKIP_TESTS == "false" }
        //       }
        //       steps {
        //         withGithubNotify(context: 'Tests', tab: 'tests') {
        //           deleteDir()
        //           unstash 'source'
        //           withGoEnv(version: "${GO_VERSION}"){
        //             dir(BASE_DIR){
        //               sh script: '.ci/scripts/build-test.sh', label: 'Build and test'
        //             }
        //           }
        //         }
        //       }
        //       post {
        //         always {
        //           junit(allowEmptyResults: true, keepLongStdio: true, testResults: "${BASE_DIR}/outputs/TEST-unit-*.xml")
        //           archiveArtifacts allowEmptyArchive: true, artifacts: "${BASE_DIR}/outputs/TEST-unit-*.xml"
        //         }
        //       }
        //     }
        //   }
        // }
        // stage('Build Docs') {
        //   options { skipDefaultCheckout() }
        //   when {
        //     beforeAgent true
        //     anyOf {
        //       expression { return env.FORCE_SKIP_GIT_CHECKS == "true" }
        //       expression { return env.SKIP_TESTS == "false" }
        //     }
        //   }
        //   steps {
        //     deleteDir()
        //     unstash 'source'
        //     dockerLogin(secret: "${DOCKER_ELASTIC_SECRET}", registry: "${DOCKER_REGISTRY}")
        //     dir("${BASE_DIR}/e2e") {
        //       sh(label: 'Build docs', script: 'make build-docs')
        //     }
        //   }
        //   post {
        //     always {
        //       dir("${BASE_DIR}") {
        //         archiveArtifacts allowEmptyArchive: true, artifacts: "e2e/docs/**"
        //       }
        //     }
        //   }
        // }
        stage('End-To-End Tests') {
          failFast true
          options { skipDefaultCheckout() }
          environment {
            GO111MODULE = 'on'
            PATH = "${env.HOME}/bin:${env.WORKSPACE}/${env.BASE_DIR}/bin:${HOME}/go/bin:${env.PATH}"
          }
          when {
            beforeAgent true
            anyOf {
              expression { return env.FORCE_SKIP_GIT_CHECKS == "true" }
              expression { return env.SKIP_TESTS == "false" }
            }
          }
          steps {
            withGithubNotify(context: 'E2E Tests', tab: 'tests') {
              deleteDir()
              unstash 'source'
              dir("${BASE_DIR}") {
                script {
                  def suitesParam = params.runTestsSuites
                  def existingSuites = readYaml(file: "${TEST_MATRIX_FILE}")
                  def parallelTasks = [:]

                  if (suitesParam == "") {
                    log(level: 'DEBUG', text: "Iterate through existing test suites")
                    existingSuites['SUITES'].each { item ->
                      checkTestSuite(parallelTasks, item)
                    }
                  } else {
                    log(level: 'DEBUG', text: "Iterate through the comma-separated test suites (${suitesParam}), comparing with the existing test suites")
                    suitesParam.split(',').each { suiteParam ->
                      existingSuites['SUITES'].findAll { suiteParam.trim() == it.suite }.each { item ->
                        checkTestSuite(parallelTasks, item)
                      }
                    }
                  }
                  parallel(parallelTasks)
                }
              }
            }
          }
          post {
            always {
              dir("${BASE_DIR}") {
                script {
                  teardownStackDocker()
                }
              }
            }
          }
        }
        // stage('Release') {
        //   options { skipDefaultCheckout() }
        //   when { tag "v*" }
        //   steps {
        //     deleteDir()
        //     unstash 'source'
        //     dir("${BASE_DIR}") {
        //       setEnvVar("GITHUB_TOKEN", getGithubToken())
        //       retryWithSleep(retries: 2, seconds: 5, backoff: true) {
        //         sh(label: 'Release binaries with gorelease', script: 'curl -sL https://git.io/goreleaser | bash -s -- --rm-dist', returnStatus: true)
        //       }
        //     }
        //   }
        //   post {
        //     always {
        //       archiveArtifacts allowEmptyArchive: true, artifacts: "${BASE_DIR}/cli/dist/**"
        //     }
        //   }
        // }
      }
    }
  }
  post {
    cleanup {
      doNotifyBuildResult(params.notifyOnGreenBuilds)
    }
  }
}

def sshexec(address, username, sshpublickey, sshprivatekey, cmd){
  sh "ssh -tt -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ${sshprivatekey} ${username}@${address} -- '${cmd}'"
}

def envToCliArgs(envContext){
  def cliOpts = []
  for (env in envContext){
    (k, v) = env.split("=")
    cliOpts.add("-var ${k.toLowerCase()}=\"${v}\"")
  }
  cliOptsStr = cliOpts.join(" ")
  return cliOptsStr
}

/*
 * Parses arguments to be utilized in the functional test steps
 */
def processFunctionalTestConfig(Map args = [:]){
  def name = args.get('name')
  def platform = args.get('platform')
  def suite = args.get('suite')
  def features = args.get('features') ? args.get('features') : []
  def tags = args.get('tags') ? args.get('tags') : ''
  def pullRequestFilter = args.get('pullRequestFilter')?.trim() ? args.get('pullRequestFilter') : ''
  def endpointMap = args.get('endpointMap') ? args.get('endpointMap') : [:]
  def platformLabels = platform.labels
  def platformProvider = platform.provider
  // Setup environment for platform
  def envContext = []

  log(level: 'INFO', text: "Building environment map for ${platformLabels}:${suite}:${tags}")

  if (!platformProvider || !platformProvider?.trim()) {
    // default provider is Docker
    platformProvider = "docker"
  }

  // We will decide whether to include the nightly tests in the execution at CI time, only.
  // On the other hand, the developers can use the TAGS environment variable locally.
  // Finally, we positively know that tags are not empty, so we can use AND operator.
  def excludeNightlyTag = " && ~${NIGHTLY_TAG}"
  if ("${NIGHTLY_SCENARIOS}" == "true") {
    excludeNightlyTag = ""
  }
  tags += excludeNightlyTag

  if (isPR() || isUpstreamTrigger(filter: 'PR-')) {
    // when the "Run_As_Master_Branch" param is disabled, we will honour the PR filters, which
    // basically exclude some less frequent platforms or operative systems. If the user enabled
    // this param, the pipeline will remove the filters from the test runner.
    if (!params.Run_As_Master_Branch) {
      tags += pullRequestFilter
    }
  }

  if(suite != "fleet"){
    platformLabels = "${platformLabels} && immutable"
  }

  def goArch = "amd64"
  if (platformLabels.contains("arm64") || platformLabels.contains("aarch64")) {
      goArch = "arm64"
  }

  if (platformLabels.contains("windows")) {
    tags += " && ~@skip:windows"
  }

  // Skip scenarios per platform
  tags = " && ~@skip:${goArch}"

  // Build environment map
  envContext.add("GOARCH=${goArch}")
  envContext.add("PROVIDER_TYPE=${platformProvider}")
  envContext.add("OP_LOG_LEVEL=TRACE")
  envContext.add("LOG_LEVEL=TRACE")

  if (platformProvider == "remote" && endpointMap) {
    envContext.add("KIBANA_URL=${endpointMap.kibanaUrl}")
    envContext.add("ELASTICSEARCH_URL=${endpointMap.elasticsearchUrl}")
    envContext.add("FLEET_URL=${endpointMap.fleetUrl}")
    envContext.add("KIBANA_PASSWORD=${endpointMap.elasticsearchPassword}")
    envContext.add("ELASTICSEARCH_PASSWORD=${endpointMap.elasticsearchPassword}")
    envContext.add("SKIP_PULL=1")
    envContext.add("TAGS=${tags}")
    envContext.add("SUITE=${suite}")
  }


  return [goArch: goArch,
          name: name,
          platform: platform,
          suite: suite,
          tags: tags,
          features: features,
          platformLabels: platformLabels,
          platformProvider: platformProvider,
          pullRequestFilter: pullRequestFilter,
          endpointMap: endpointMap,
          envContext: envContext]
}

/*
 * Allows provisioning of machines via terraform
 */
def runTF(profile, args){
  def binDir = "${HOME}/bin"
  def props = getVaultSecret(secret: "${ELASTIC_CLOUD_SECRET}")
  def authObj = props?.data
  def awsProps = getVaultSecret(secret: "${AWS_PROVISIONER_SECRET}")
  def awsAuthObj = awsProps?.data
  withEnv([
    "PATH=${PATH}:${binDir}"
  ]){
    // sh(label:'Install Terraform', script: """
    //   mkdir -p ${binDir} || true
    //   cd ${binDir}
    //   curl -sSL -o terraform.zip https://releases.hashicorp.com/terraform/1.0.9/terraform_1.0.9_linux_amd64.zip || true
    //   unzip terraform.zip || true
    // """)
    withVaultToken(){
      withGCPEnv(secret: "${GCP_PROVISIONER_SECRET}"){
        withEnvMask(vars: [
          [var: "EC_API_KEY", password: authObj.apiKey],
          [var: "AWS_ACCESS_KEY_ID", password: awsAuthObj.access_key],
          [var: "AWS_SECRET_ACCESS_KEY", password: awsAuthObj.secret_key]
        ]) {
          return sh(script: """docker run
                -v "${env.GOOGLE_APPLICATION_CREDENTIALS}":/gcp/creds.json:ro
                -v "${env.WORKSPACE}/${env.BASE_DIR}":/e2e-testing
                -w /e2e-testing/.ci/terraform/${profile}
                --env GOOGLE_APPLICATION_CREDENTIALS=/gcp/creds.json
                --env AWS_ACCESS_KEY_ID=${env.AWS_ACCESS_KEY_ID}
                --env AWS_SECRET_ACCESS_KEY=${env.AWS_SECRET_ACCESS_KEY}
                hashicorp/terraform:latest ${args}
             """.replaceAll('\\s+', ' '), returnStdout: true)?.trim()
        }
      }
    }
  }
}

def deployStack(){
  def endpointMap = [:]
  def workDir = ".ci/terraform/stack"
  runTF("stack", "init")
  runTF("stack", "apply -auto-approve")
  elasticsearchAddress = sh(label: 'Get ES endpoint',
                            script: "echo ec_deployment.end-to-end.elasticsearch[0].https_endpoint | terraform -chdir=${workDir} console",
                            returnStdout: true)?.trim()
  kibanaAddress = sh(label: 'Get Kibana endpoint',
                     script: "echo ec_deployment.end-to-end.kibana[0].https_endpoint | terraform -chdir=${workDir} console",
                     returnStdout: true)?.trim()
  fleetAddress = sh(label: 'Get Fleet endpoint',
                    script: "echo ec_deployment.end-to-end.apm[0].https_endpoint | terraform -chdir=${workDir} console",
                    returnStdout: true)?.trim()
  endpointPassword = sh(label: 'Get password',
                        script: "terraform -chdir=${workDir} show -json | jq -r .values.root_module.resources[0].values.elasticsearch_password",
                        returnStdout: true)?.trim()
  endpointMap = [elasticsearchUrl: elasticsearchAddress,
                 kibanaUrl: kibanaAddress,
                 fleetUrl: fleetAddress.replace(".apm", ".fleet"),
                 elasticsearchPassword: endpointPassword]
  return endpointMap
}


/*
 * Placeholder deployment until we are able to work out specifying snapshots in staging cloud.
 * This will deploy a docker stack on a separate gcp instance and be exposed to the internet
 * for testing across cloud environments.
 */
def deployStackDocker(){
  sh "ssh-keygen -b 4096 -t rsa -f ${env.BASE_DIR}/e2essh -q -N \"\" "

  def endpointMap = [:]
  def cliOptsStr = envToCliArgs([
    "privatekeypath=\"${env.WORKSPACE}/e2essh\"",
    "publickeypath=\"${env.WORKSPACE}/e2essh.pub\"",
    "workspace=\"${env.WORKSPACE}\"",
    "base_dir=\"${env.BASE_DIR}\"",
  ])

  def nodeAddress = "127.0.0.1"
  runTF("stack-docker", "init")
  runTF("stack-docker", "apply -auto-approve ${cliOptsStr}")
  nodeAddress = runTF("stack-docker", "output -raw ip")
  endpointMap = [elasticsearchUrl: "http://${nodeAddress}:9200",
                 kibanaUrl: "http://${nodeAddress}:5601",
                 fleetUrl: "http://${nodeAddress}:8220",
                 elasticsearchPassword: "changeme"]

  return endpointMap
}

/*
 * Teardown docker stack
 */
def teardownStackDocker(){
  def cliOptsStr = envToCliArgs([
    "privatekeypath=\"${env.WORKSPACE}/e2essh\"",
    "publickeypath=\"${env.WORKSPACE}/e2essh.pub\"",
    "workspace=\"${env.WORKSPACE}\"",
    "base_dir=\"${env.BASE_DIR}\"",
  ])

  runTF("stack-docker", "init")
  runTF("stack-docker", "destroy -auto-approve ${cliOptsStr}")
}

def deployNode(opts, uuid){
  def nodeAddress = null
  def nodeUser = "ci"
  def sshKeyDir = "${env.WORKSPACE}/${uuid}"
  def sshKeyFname = "e2essh-${uuid}"
  def sshKeyPath = "${sshKeyDir}/${sshKeyFname}"
  def envMap = opts.envContext.clone()

  envMap.add("privatekeypath=\"${sshKeyPath}\"")
  envMap.add("publickeypath=\"${sshKeyPath}.pub\"")
  envMap.add("workspace=\"${env.WORKSPACE}\"")
  envMap.add("base_dir=\"${BASE_DIR}\"")

  def cliOptsStr = envToCliArgs(envMap)

  sh "mkdir -p ${sshKeyDir} || true"
  sh "ssh-keygen -b 4096 -t rsa -f ${sshKeyPath} -q -N \"\" || true"
  runTF(opts.platformLabels, "init")
  runTF(opts.platformLabels, "apply -auto-approve ${cliOptsStr}")
  nodeAddress = runTF(opts.platformLabels, "output -raw ip")
  nodeUser = runTF(opts.platformLabels, "output -raw username")
  return [nodeAddress: nodeAddress, nodeUser: nodeUser, sshPrivateKey: "${sshKeyPath}", sshPublicKey: "${sshKeyPath}.pub"]
}

/*
 * Iterate through known terraform configurations and attempt to bring down any deployed resources
 */
def teardownNode(opts, uuid){
  def sshKeyDir = "${env.WORKSPACE}/${uuid}"
  def sshKeyFname = "e2essh-${uuid}"
  def sshKeyPath = "${sshKeyDir}/${sshKeyFname}"
  def envMap = opts.envContext.clone()

  envMap.add("privatekeypath=\"${sshKeyPath}\"")
  envMap.add("publickeypath=\"${sshKeyPath}.pub\"")
  envMap.add("workspace=\"${env.WORKSPACE}\"")
  envMap.add("base_dir=\"${BASE_DIR}\"")

  def cliOptsStr = envToCliArgs(envMap)

  log(level: 'INFO', text: "Destroying ${opts.platformLabels} machine")
  runTF(opts.platformLabels, "init")
  runTF(opts.platformLabels, "destroy -auto-approve ${cliOptsStr}")
}

def checkTestSuite(Map parallelTasks = [:], Map item = [:]) {
  def suite = item.suite
  def platforms = item.platforms
  def endpointMap = [:]

  // Start stack with fleet server enabled for the fleet suite of tests
  if (suite == "fleet") {
    endpointMap = deployStackDocker()
  }
  if(isInstalled(tool: 'docker', flag: '--version')) {
    dockerLogin(secret: "${DOCKER_ELASTIC_SECRET}", registry: "${DOCKER_REGISTRY}")
  }

  item.scenarios.each { scenario ->
    def name = scenario.name
    def platformsValue = platforms
    def scenarioPlatforms = scenario.platforms
    if (scenarioPlatforms?.size() > 0) {
      // scenario platforms take precedence over suite platforms, overriding them
      platformsValue = scenarioPlatforms
    }
    def pullRequestFilter = scenario.containsKey('pullRequestFilter') ? scenario.pullRequestFilter : ''
    def tags = scenario.containsKey('tags') ? scenario.tags : ''
    def features = scenario.features
    def regexps = [ "^e2e/_suites/${suite}/.*", "^.ci/.*", "^cli/.*", "^e2e/.*\\.go", "^internal/.*\\.go" ]
    if ("${FORCE_SKIP_GIT_CHECKS}" == "true" || isGitRegionMatch(patterns: regexps, shouldMatchAll: false)) {
      platformsValue.each { platform ->
        opts = processFunctionalTestConfig(name: "${name}",
                                           platform: platform,
                                           suite: "${suite}",
                                           tags: "${tags}",
                                           features: features,
                                           pullRequestFilter: "${pullRequestFilter}",
                                           endpointMap: endpointMap)
        log(level: 'INFO', text: "Adding ${opts.platformLabels}:${opts.suite} test suite to the build execution")
        if(suite == "fleet"){
          opts.features.each { feature ->
            parallelTasks["${opts.platformLabels}_${opts.suite}_${feature}"] = generateFunctionalFleetTestStep(opts, feature)
          }
        } else {
          parallelTasks["${opts.platformLabels}_${opts.suite}_${opts.tags}"] = generateFunctionalTestStep(opts)
        }
      }
    } else {
      log(level: 'WARN', text: "The ${suite}:${tags} test suite won't be executed in any platform because there are no modified files")
    }
  }
}

/*
 * Sends out notification of the build result to Slack
 */
def doNotifyBuildResult(boolean slackNotify) {
  githubCheckNotify(currentBuild.currentResult == 'SUCCESS' ? 'SUCCESS' : 'FAILURE')

  def testsSuites = "${params.runTestsSuites}"
  if (testsSuites?.trim() == "") {
    testsSuites = "All suites"
  }

  def channels = "${env.SLACK_CHANNEL}"
  if (channels?.trim() == "") {
    channels = "observablt-bots"
  }

  def header = "*Test Suite*: " + testsSuites
  notifyBuildResult(analyzeFlakey: true,
                    jobName: getFlakyJobName(withBranch: "${env.JOB_BASE_NAME}"),
                    prComment: true,
                    slackHeader: header,
                    slackChannel: "${channels}",
                    slackComment: true,
                    slackNotify: slackNotify)
}

/**
 * Tear down the setup for the static workers.
 */
def tearDown(Map args = [:]){
  catchError(buildResult: 'SUCCESS', stageResult: 'SUCCESS') {
    dir("${BASE_DIR}"){
      sh(label: 'Remove the entire module cache', script: 'go clean -modcache', returnStatus: true)
    }
    if (isStaticWorker(labels: args.labels)) {
      dir("${WORKSPACE}") {
        deleteDir()
      }
    }
  }
}

/**
 Notify the GitHub check of the parent stream
 **/
def githubCheckNotify(String status) {
  if (params.GITHUB_CHECK_NAME?.trim() && params.GITHUB_CHECK_REPO?.trim() && params.GITHUB_CHECK_SHA1?.trim()) {
    githubNotify context: "${params.GITHUB_CHECK_NAME}",
      description: "${params.GITHUB_CHECK_NAME} ${status.toLowerCase()}",
      status: "${status}",
      targetUrl: "${env.RUN_DISPLAY_URL}",
      sha: params.GITHUB_CHECK_SHA1, account: 'elastic', repo: params.GITHUB_CHECK_REPO, credentialsId: env.JOB_GIT_CREDENTIALS
  }
}

/*
 * Generates the functional step for all other Suites
 */
def generateFunctionalTestStep(opts){
  // Setup environment for platform
  opts.envContext.add("PROVIDER=${opts.platformProvider}")
  opts.envContext.add("ELASTIC_APM_GLOBAL_LABELS=branch_name=${BRANCH_NAME},build_pr=${isPR()},build_id=${env.BUILD_ID},go_arch=${opts.goArch},beat_version=${env.BEAT_VERSION},stack_version=${env.STACK_VERSION}")

  return {
    withNode(labels: "${opts.platformLabels}", sleepMax: 20, forceWorkspace: true){
      deleteDir()
      unstash 'source'
      try {
        withDockerEnv(secret: "${DOCKER_ELASTIC_SECRET}", registry: "${DOCKER_REGISTRY}") {
          filebeat(output: "docker_logs_${opts.platformLabels}_${opts.goArch}_${opts.suite}_${opts.name}.log", workdir: "${env.WORKSPACE}") {
            withEnv(opts.envContext) {
              withGoEnv(version: "${GO_VERSION}") {
                // This step will help to send the APM traces to the
                // APM service defined by the Otel Jenkins plugin.
                withOtelEnv() {
                  sh script: """.ci/scripts/functional-test.sh "${opts.suite}" "${opts.tags}" "${STACK_VERSION}" "${BEAT_VERSION}" """, label: "Run functional tests for ${opts.platformLabels}:${opts.suite}:${opts.tags}"
                }
              }
            }
          }
        }
      } finally {
        junit(allowEmptyResults: true, keepLongStdio: true, testResults: "${BASE_DIR}/outputs/TEST-*.xml")
        archiveArtifacts allowEmptyArchive: true, artifacts: "${BASE_DIR}/outputs/TEST-*.xml"
        tearDown(labels: opts.platformLabels)
      }
    }
  }
}

/*
 * Generates the functional step for testing Fleet via Terraform
 */
def generateFunctionalFleetTestStep(opts, feature){
  return {
    // withNode(labels: "ubuntu-20.04 && immutable && docker", sleepMax: 20, forceWorkspace: true, forceWorker: true){
    uuid = UUID.randomUUID().toString().split("-")[0]
    nodeDeployResult = deployNode(opts, uuid)
    try {
      log(level: 'INFO', text: "Starting ${opts.platformLabels}:${opts.suite}:${feature} ${nodeDeployResult.nodeAddress}")
      tags = "${feature}${opts.tags}"
      sshexec(nodeDeployResult.nodeAddress,
              nodeDeployResult.nodeUser,
              nodeDeployResult.sshPublicKey,
              nodeDeployResult.sshPrivateKey,
              """sudo -E bash /home/${nodeDeployResult.nodeUser}/e2e-testing/.ci/scripts/functional-test.sh "${opts.suite}" "${tags}" "${STACK_VERSION}" "${BEAT_VERSION}" """)
      log(level: 'INFO', text: "Finished ${opts.platformLabels}:${opts.suite}:${feature} ${nodeDeployResult.nodeAddress}")
    } finally {
      sh "mkdir -p ${BASE_DIR}/outputs || true"
      sh "scp -r -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ${nodeDeployResult.sshPrivateKey} ${nodeDeployResult.nodeUser}@${nodeDeployResult.nodeAddress}:/home/${nodeDeployResult.nodeUser}/e2e-testing/outputs/* ${BASE_DIR}/outputs/. || true"
      teardownNode(opts, uuid)
      junit(allowEmptyResults: true, keepLongStdio: true, testResults: "${BASE_DIR}/outputs/TEST-*.xml")
      archiveArtifacts allowEmptyArchive: true, artifacts: "${BASE_DIR}/outputs/TEST-*.xml"
    }
    // }
  }
}
